# -*- coding: utf-8 -*-
"""Industrial Project (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VqQg7vfFROYG2jpY40QqvjIzJfwNe6BZ
"""

import warnings, gc
import numpy as np
import pandas as pd
import matplotlib.colors
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from plotly.offline import init_notebook_mode
from datetime import datetime, timedelta
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error,mean_absolute_error
from lightgbm import LGBMRegressor
from decimal import ROUND_HALF_UP, Decimal
warnings.filterwarnings("ignore")
import plotly.figure_factory as ff
import matplotlib.pyplot as plt

init_notebook_mode(connected=True)
temp = dict(layout=go.Layout(font=dict(family="Franklin Gothic", size=12), width=800))
colors=px.colors.qualitative.Plotly
path_to_file="/Users/Pan/Downloads/j/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv"
path_to_file2="/Users/Pan/Downloads/j/jpx-tokyo-stock-exchange-prediction/stock_list.csv"
train=pd.read_csv(path_to_file, parse_dates=['Date'])
stock_list=pd.read_csv(path_to_file2)
stock_list['SectorName']=[i.rstrip().lower().capitalize() for i in stock_list['17SectorName']]
stock_list['Name']=[i.rstrip().lower().capitalize() for i in stock_list['Name']]
train2_df = train.merge(stock_list[['SecuritiesCode','Name','SectorName']], on='SecuritiesCode', how='left')
train2_df['Year'] = train2_df['Date'].dt.year
train2_df['Month'] = train2_df['Date'].dt.month
years = {year: pd.DataFrame() for year in train2_df.Year.unique()[::-1]}
months = {month: pd.DataFrame() for month in train2_df.Month.unique()[::-1]}
train2_df=train2_df[train2_df["SectorName"]=="Energy resources"]
train2_df
print("The training data begins on {} and ends on {}.\n".format(train2_df.Date.min(),train2_df.Date.max()))
display(train2_df.describe().style.format('{:,.2f}'))

train2_df

train2_df.head(28)

def create_plot(stocks):

    for stock in stocks:
        # Create a column for the adjusted close of each stock
        # Here we use the DataReader library to get the data.
        train2_df[stock['SecuritiesCode']] = wb.DataReader(stock['ticker'], data_source='yahoo', start='2007-1-1')['Adj Close']

train_date=train2_df.Date.unique()

close_avg=train2_df.groupby('Date')['Close']



for i, j in enumerate([close_avg]):
    fig.add_trace(go.Scatter(x=train_date, y=j, mode='lines',
                             name=j.name, marker_color=colors[i]), row=i+1, col=1)
fig.update_xaxes(rangeslider_visible=False,
                 rangeselector=dict(
                     buttons=list([dict(step="all")])),
                 row=1,col=1)
fig.update_layout(template=temp,title='Energy Resources Average Stock Return, Closing Price, and Shares Traded',
                  hovermode='x unified', height=500,
                  yaxis1=dict(title='Stock Return'),
                  yaxis2_title='Closing Price', yaxis3_title='Shares Traded',
                  showlegend=False)
fig.show()

for key in years.keys():
    df=train2_df[train2_df.Year == key]
    years[key] = df.groupby('Name')['Target'].mean().mul(100).rename("Avg_return_{}".format(key))
df=pd.concat((years[i].to_frame() for i in years.keys()), axis=1)
df=df.sort_values(by="Avg_return_2021")

fig = make_subplots(rows=1, cols=5, shared_yaxes=True)
for i, col in enumerate(df.columns):
    x = df[col]
    mask = x<=0
    fig.add_trace(go.Bar(x=x[mask], y=df.index[mask],orientation='h',
                         text=x[mask], texttemplate='%{text:.2f}%',textposition='auto',
                         hovertemplate='Average Return in %{y} Stocks = %{x:.4f}%',
                         marker=dict(color='red', opacity=0.7),name=col[-4:]),
                  row=1, col=i+1)
    fig.add_trace(go.Bar(x=x[~mask], y=df.index[~mask],orientation='h',
                         text=x[~mask], texttemplate='%{text:.2f}%', textposition='auto',
                         hovertemplate='Average Return in %{y} Stocks = %{x:.4f}%',
                         marker=dict(color='green', opacity=0.7),name=col[-4:]),
                  row=1, col=i+1)
    fig.update_xaxes(range=(x.min()-.15,x.max()+.15), title='{} Returns'.format(col[-4:]),
                     showticklabels=False, row=1, col=i+1)
fig.update_layout(template=temp,title='Yearly Average Stock Returns',
                  hovermode='closest',margin=dict(l=250,r=50),
                  height=600, width=1000, showlegend=False)
fig.show()

train_df=train2_df[train2_df.Date>'2020-12-23']
print("New Train Shape {}.\nMissing values in Target = {}".format(train_df.shape,train_df['Target'].isna().sum()))

target_std_per_date = train2_df.groupby(['Date'])['Target'].std()
target_std_mean = target_std_per_date.mean()

fig, ax = plt.subplots(figsize=(10, 5))
sns.histplot(data=target_std_per_date.values, bins=100, kde=True,
             ax=ax)
ax.axvline(x=target_std_mean, color='orange', linestyle='dotted', linewidth=2,
           label='Mean')
ax.set_title("Target Std Distibution\n"
             f"Min {round(target_std_per_date.min(), 4)} | "
             f"Max {round(target_std_per_date.max(), 4)} | "
             f"Skewness {round(target_std_per_date.skew(), 2)} | "
             f"Kurtosis {round(target_std_per_date.kurtosis(), 2)}")
ax.set_xlabel("Target Std")
ax.set_ylabel("Date Count")
ax.legend()
plt.show()

target_mean_per_date = train2_df.groupby(['Date'])['Target'].mean()
target_mean_mean = target_mean_per_date.mean()

fig, ax = plt.subplots(figsize=(10, 5))
sns.histplot(data=target_mean_per_date.values, bins=100, kde=True,
             ax=ax)
ax.axvline(x=target_mean_mean, color='orange', linestyle='dotted', linewidth=2,
           label='Mean')
ax.set_title("Target Mean Distibution\n"
             f"Min {round(target_mean_per_date.min(), 4)} | "
             f"Max {round(target_mean_per_date.max(), 4)} | "
             f"Skewness {round(target_mean_per_date.skew(), 2)} | "
             f"Kurtosis {round(target_mean_per_date.kurtosis(), 2)}")
ax.set_xlabel("Target Mean")
ax.set_ylabel("Date Count")
ax.legend()
plt.show()

pal = ['hsl('+str(h)+',50%'+',50%)' for h in np.linspace(0, 360, 18)]
fig = go.Figure()
for i, sector in enumerate(df.index[::-1]):
    y_data=train2_df[train2_df['Name']==sector]['Target']
    fig.add_trace(go.Box(y=y_data*100, name=sector,
                         marker_color=pal[i], showlegend=False))
fig.update_layout(template=temp, title='Target Distribution by each stock',
                  yaxis=dict(title='Stock Return',ticksuffix='%'),
                  margin=dict(b=150), height=750, width=900)
fig.show()

train_date=train2_df.Date.unique()
sectors=train2_df.SectorName.tolist()
open_avg=train2_df.groupby('Date')['Open'].mean()
high_avg=train2_df.groupby('Date')['High'].mean()
low_avg=train2_df.groupby('Date')['Low'].mean()
close_avg=train2_df.groupby('Date')['Close'].mean()
buttons=[]

fig = go.Figure()
for i in range(18):
    if i != 0:
        open_avg=train2_df[train2_df.SectorName==sectors[i]].groupby('Date')['Open'].mean()
        high_avg=train2_df[train2_df.SectorName==sectors[i]].groupby('Date')['High'].mean()
        low_avg=train2_df[train2_df.SectorName==sectors[i]].groupby('Date')['Low'].mean()
        close_avg=train2_df[train2_df.SectorName==sectors[i]].groupby('Date')['Close'].mean()

    fig.add_trace(go.Candlestick(x=train_date, open=open_avg, high=high_avg,
                                 low=low_avg, close=close_avg, name=sectors[i],
                                 visible=(True if i==0 else False)))

    visibility=[False]*len(sectors)
    visibility[i]=True
fig.update_xaxes(rangeslider_visible=True,
                 rangeselector=dict(
                     buttons=list([
                         dict(count=3, label="3m", step="month", stepmode="backward"),
                         dict(count=6, label="6m", step="month", stepmode="backward"),
                         dict(step="all")]), xanchor='left',yanchor='bottom', y=1.16, x=.01))
fig.update_layout(template=temp,title='Energy Resources Stock Price Movements',
                  hovermode='x unified', showlegend=False, width=1000,
                  updatemenus=[dict(active=0, type="dropdown",
                                    buttons=buttons, xanchor='left',
                                    yanchor='bottom', y=1.01, x=.01)],
                  yaxis=dict(title='Stock Price'))
fig.show()

stock=train_df.groupby('Name')['Target'].mean().mul(100)
stock_low=stock.nsmallest(7)[::-1].rename("Return")
stock_high=stock.nlargest(7).rename("Return")
stock=pd.concat([stock_high, stock_low], axis=0).reset_index()
stock['Sector']='All'
for i in train_df.SectorName.unique():
    sector=train2_df[train2_df.SectorName==i].groupby('Name')['Target'].mean().mul(100)
    stock_low=sector.nsmallest(7)[::-1].rename("Return")
    stock_high=sector.nlargest(7).rename("Return")
    sector_stock=pd.concat([stock_high, stock_low], axis=0).reset_index()
    sector_stock['Sector']=i
    stock=stock.append(sector_stock,ignore_index=True)

fig=go.Figure()
buttons = []
for i, sector in enumerate(stock.Sector.unique()):

    x=stock[stock.Sector==sector]['Name']
    y=stock[stock.Sector==sector]['Return']
    mask=y>0
    fig.add_trace(go.Bar(x=x[mask], y=y[mask], text=y[mask],
                         texttemplate='%{text:.2f}%',
                         textposition='auto',
                         name=sector, visible=(False if i != 0 else True),
                         hovertemplate='%{x} average return: %{y:.3f}%',
                         marker=dict(color='green', opacity=0.7)))
    fig.add_trace(go.Bar(x=x[~mask], y=y[~mask], text=y[~mask],
                         texttemplate='%{text:.2f}%',
                         textposition='auto',
                         name=sector, visible=(False if i != 0 else True),
                         hovertemplate='%{x} average return: %{y:.3f}%',
                         marker=dict(color='red', opacity=0.7)))

    visibility=[False]*2*len(stock.Sector.unique())
    visibility[i*2],visibility[i*2+1]=True,True
    button = dict(label = sector,
                  method = "update",
                  args=[{"visible": visibility}])
    buttons.append(button)

fig.update_layout(title='Stocks with Highest and Lowest Returns by Sector',
                  template=temp, yaxis=dict(title='Average Return', ticksuffix='%'),
                  updatemenus=[dict(active=0, type="dropdown",
                                    buttons=buttons, xanchor='left',
                                    yanchor='bottom', y=1.01, x=.01)],
                  margin=dict(b=150),showlegend=False,height=700, width=900)
fig.show()

stocks=train2_df[train2_df.SecuritiesCode.isin([1518,1605,3315,5017])]
df_pivot=stocks.pivot_table(index='Date', columns='Name', values='Close').reset_index()
pal=['rgb'+str(i) for i in sns.color_palette("coolwarm", len(df_pivot))]

fig = ff.create_scatterplotmatrix(df_pivot.iloc[:,1:], diag='histogram', name='')
fig.update_traces(marker=dict(color=pal, opacity=0.9, line_color='white', line_width=.5))
fig.update_layout(template=temp, title='Scatterplots of Highest Performing Stocks',
                  height=1000, width=1000, showlegend=False)
fig.show()

stocks=train2_df[train2_df.SecuritiesCode.isin([1662,5019,5021,5020])]
df_pivot=stocks.pivot_table(index='Date', columns='Name', values='Close').reset_index()
pal=['rgb'+str(i) for i in sns.color_palette("coolwarm", len(df_pivot))]

fig = ff.create_scatterplotmatrix(df_pivot.iloc[:,1:], diag='histogram', name='')
fig.update_traces(marker=dict(color=pal, opacity=0.9, line_color='white', line_width=.5))
fig.update_layout(template=temp, title='Scatterplots of Highest Performing Stocks',
                  height=1000, width=1000, showlegend=False)
fig.show()

stocks=train2_df[train2_df.SecuritiesCode.isin([5008,5015,1515,1663,5013])]
df_pivot=stocks.pivot_table(index='Date', columns='Name', values='Close').reset_index()
pal=['rgb'+str(i) for i in sns.color_palette("coolwarm", len(df_pivot))]

fig = ff.create_scatterplotmatrix(df_pivot.iloc[:,1:], diag='histogram', name='')
fig.update_traces(marker=dict(color=pal, opacity=0.9, line_color='white', line_width=.5))
fig.update_layout(template=temp, title='Scatterplots of Highest Performing Stocks',
                  height=1000, width=1000, showlegend=False)
fig.show()

corr=train2_df.groupby('SecuritiesCode')[['Target','Close']].corr().unstack().iloc[:,1]
stocks=corr.nlargest(10).rename("Return").reset_index()
stocks=stocks.merge(train2_df[['Name','SecuritiesCode']], on='SecuritiesCode').drop_duplicates()
pal=sns.color_palette("magma_r", 14).as_hex()
rgb=['rgba'+str(matplotlib.colors.to_rgba(i,0.7)) for i in pal]

fig = go.Figure()
fig.add_trace(go.Bar(x=stocks.Name, y=stocks.Return, text=stocks.Return,
                     texttemplate='%{text:.2f}', name='', width=0.8,
                     textposition='outside',marker=dict(color=rgb, line=dict(color=pal,width=1)),
                     hovertemplate='Correlation of %{x} with target = %{y:.3f}'))
fig.update_layout(template=temp, title='Correlated Stocks with Target Variable',
                  yaxis=dict(title='Correlation',showticklabels=False),
                  xaxis=dict(title='Stock',tickangle=45), margin=dict(b=200),
                  width=700,height=600)
fig.show()

def adjust_price(price):
    """
    Args:
        price (pd.DataFrame)  : pd.DataFrame include stock_price
    Returns:
        price DataFrame (pd.DataFrame): stock_price with generated AdjustedClose
    """
    # transform Date column into datetime
    price.loc[: ,"Date"] = pd.to_datetime(price.loc[: ,"Date"], format="%Y-%m-%d")

    def generate_adjusted_close(df):
        """
        Args:
            df (pd.DataFrame)  : stock_price for a single SecuritiesCode
        Returns:
            df (pd.DataFrame): stock_price with AdjustedClose for a single SecuritiesCode
        """
        # sort data to generate CumulativeAdjustmentFactor
        df = df.sort_values("Date", ascending=False)
        # generate CumulativeAdjustmentFactor
        df.loc[:, "CumulativeAdjustmentFactor"] = df["AdjustmentFactor"].cumprod()
        # generate AdjustedClose
        df.loc[:, "AdjustedClose"] = (
            df["CumulativeAdjustmentFactor"] * df["Close"]
        ).map(lambda x: float(
            Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)
        ))
        # reverse order
        df = df.sort_values("Date")
        # to fill AdjustedClose, replace 0 into np.nan
        df.loc[df["AdjustedClose"] == 0, "AdjustedClose"] = np.nan
        # forward fill AdjustedClose
        df.loc[:, "AdjustedClose"] = df.loc[:, "AdjustedClose"].ffill()
        return df

    # generate AdjustedClose
    price = price.sort_values(["SecuritiesCode", "Date"])
    price = price.groupby("SecuritiesCode").apply(generate_adjusted_close).reset_index(drop=True)
    return price

train=train2_df.drop('ExpectedDividend',axis=1).fillna(0)
prices=adjust_price(train)

prices

def create_features(df):
    df=df.copy()
    col='AdjustedClose'
    periods=[5,10,30,50]
    for period in periods:
        df.loc[:,"Return_{}Day".format(period)] = df.groupby("SecuritiesCode")[col].pct_change(period)
        df.loc[:,"MovingAvg_{}Day".format(period)] = df.groupby("SecuritiesCode")[col].rolling(window=period).mean().values
        df.loc[:,"ExpMovingAvg_{}Day".format(period)] = df.groupby("SecuritiesCode")[col].ewm(span=period,adjust=False).mean().values
        df.loc[:,"Volatility_{}Day".format(period)] = np.log(df[col]).groupby(df["SecuritiesCode"]).diff().rolling(period).std()
    return df

price_features=create_features(df=prices)
price_features.drop(['RowId','SupervisionFlag','AdjustmentFactor','CumulativeAdjustmentFactor'],axis=1,inplace=True)

price_features

price_names=price_features.set_index('Date')
price_names=price_names[price_names.index>='2020-01-04']
price_names.fillna(0, inplace=True)

features=['MovingAvg','ExpMovingAvg','Return', 'Volatility']
names=['Average', 'Exp. Moving Average', 'Period', 'Volatility']
buttons=[]

fig = make_subplots(rows=2, cols=2,
                    shared_xaxes=True,
                    vertical_spacing=0.1,
                    subplot_titles=('Adjusted Close Moving Average',
                                    'Exponential Moving Average',
                                    'Stock Return', 'Stock Volatility'))

price_names

price_names=price_features.set_index('Date')
price_names=price_names[price_names.index>='2017-01-04']
price_names=price_names[price_names.index<='2018-12-31']
price_names.fillna(0, inplace=True)

features=['MovingAvg','ExpMovingAvg','Return', 'Volatility']
names=['Average', 'Exp. Moving Average', 'Period', 'Volatility']
buttons=[]

fig = make_subplots(rows=2, cols=2,
                    shared_xaxes=True,
                    vertical_spacing=0.1,
                    subplot_titles=('Adjusted Close Moving Average',
                                    'Exponential Moving Average',
                                    'Stock Return', 'Stock Volatility'))

for i, sector in enumerate(price_names.Name.unique()):

    sector_df=price_names[price_names.Name==sector]
    periods=[5,10,30]
    colors=px.colors.qualitative.Vivid
    dash=['solid','dash', 'longdash', 'dashdot', 'longdashdot']
    row,col=1,1

    for j, (feature, name) in enumerate(zip(features, names)):
        if j>=2:
            row,periods=2,[5,10,30]
            colors=px.colors.qualitative.Bold[1:]
        if j%2==0:
            col=1
        else:
            col=2

        for k, period in enumerate(periods):
            if (k==0)&(j<2):
                plot_data=sector_df.groupby(sector_df.index)['AdjustedClose'].mean().rename('Adjusted Close')
            elif j>=2:
                plot_data=sector_df.groupby(sector_df.index)['{}_{}Day'.format(feature,period)].mean().mul(100).rename('{}-day {}'.format(period,name))
            else:
                plot_data=sector_df.groupby(sector_df.index)['{}_{}Day'.format(feature,period)].mean().rename('{}-day {}'.format(period,name))
            fig.add_trace(go.Scatter(x=plot_data.index, y=plot_data, mode='lines',
                                     name=plot_data.name, marker_color=colors[k+1],
                                     line=dict(width=2,dash=(dash[k] if j<2 else 'solid')),
                                     showlegend=(True if (j==0) or (j==2) else False), legendgroup=row,
                                     visible=(False if i != 0 else True)), row=row, col=col)

    visibility=[False]*14*len(price_names.Name.unique())
    for l in range(i*14, i*14+14):
        visibility[l]=True
    button = dict(label = sector,
                  method = "update",
                  args=[{"visible": visibility}])
    buttons.append(button)

fig.update_layout(title='Stock Price Moving Average, Return,<br>and Volatility by Sector',
                  template=temp, yaxis3_ticksuffix='%', yaxis4_ticksuffix='%',
                  legend_title_text='Period', legend_tracegroupgap=250,
                  updatemenus=[dict(active=0, type="dropdown",
                                    buttons=buttons, xanchor='left',
                                    yanchor='bottom', y=1.105, x=.01)],
                  hovermode='x unified', height=800,width=1200, margin=dict(t=150))
fig.show()

price_names=price_features.set_index('Date')
price_names=price_names[price_names.index>'2018-12-31']
price_names=price_names[price_names.index<='2019-12-31']
price_names.fillna(0, inplace=True)

features=['MovingAvg','ExpMovingAvg','Return', 'Volatility']
names=['Average', 'Exp. Moving Average', 'Period', 'Volatility']
buttons=[]

fig = make_subplots(rows=2, cols=2,
                    shared_xaxes=True,
                    vertical_spacing=0.1,
                    subplot_titles=('Adjusted Close Moving Average',
                                    'Exponential Moving Average',
                                    'Stock Return', 'Stock Volatility'))

for i, sector in enumerate(price_names.Name.unique()):

    sector_df=price_names[price_names.Name==sector]
    periods=[5,10,30]
    colors=px.colors.qualitative.Vivid
    dash=['solid','dash', 'longdash', 'dashdot', 'longdashdot']
    row,col=1,1

    for j, (feature, name) in enumerate(zip(features, names)):
        if j>=2:
            row,periods=2,[5,10,30]
            colors=px.colors.qualitative.Bold[1:]
        if j%2==0:
            col=1
        else:
            col=2

        for k, period in enumerate(periods):
            if (k==0)&(j<2):
                plot_data=sector_df.groupby(sector_df.index)['AdjustedClose'].mean().rename('Adjusted Close')
            elif j>=2:
                plot_data=sector_df.groupby(sector_df.index)['{}_{}Day'.format(feature,period)].mean().mul(100).rename('{}-day {}'.format(period,name))
            else:
                plot_data=sector_df.groupby(sector_df.index)['{}_{}Day'.format(feature,period)].mean().rename('{}-day {}'.format(period,name))
            fig.add_trace(go.Scatter(x=plot_data.index, y=plot_data, mode='lines',
                                     name=plot_data.name, marker_color=colors[k+1],
                                     line=dict(width=2,dash=(dash[k] if j<2 else 'solid')),
                                     showlegend=(True if (j==0) or (j==2) else False), legendgroup=row,
                                     visible=(False if i != 0 else True)), row=row, col=col)

    visibility=[False]*14*len(price_names.Name.unique())
    for l in range(i*14, i*14+14):
        visibility[l]=True
    button = dict(label = sector,
                  method = "update",
                  args=[{"visible": visibility}])
    buttons.append(button)

fig.update_layout(title='Stock Price Moving Average, Return,<br>and Volatility by Sector',
                  template=temp, yaxis3_ticksuffix='%', yaxis4_ticksuffix='%',
                  legend_title_text='Period', legend_tracegroupgap=250,
                  updatemenus=[dict(active=0, type="dropdown",
                                    buttons=buttons, xanchor='left',
                                    yanchor='bottom', y=1.105, x=.01)],
                  hovermode='x unified', height=800,width=1200, margin=dict(t=150))
fig.show()

price_names=price_features.set_index('Date')
price_names=price_names[price_names.index>'2019-12-31']
price_names=price_names[price_names.index<='2020-12-31']
price_names.fillna(0, inplace=True)

features=['MovingAvg','ExpMovingAvg','Return', 'Volatility']
names=['Average', 'Exp. Moving Average', 'Period', 'Volatility']
buttons=[]

fig = make_subplots(rows=2, cols=2,
                    shared_xaxes=True,
                    vertical_spacing=0.1,
                    subplot_titles=('Adjusted Close Moving Average',
                                    'Exponential Moving Average',
                                    'Stock Return', 'Stock Volatility'))

for i, sector in enumerate(price_names.Name.unique()):

    sector_df=price_names[price_names.Name==sector]
    periods=[5,10,30]
    colors=px.colors.qualitative.Vivid
    dash=['solid','dash', 'longdash', 'dashdot', 'longdashdot']
    row,col=1,1

    for j, (feature, name) in enumerate(zip(features, names)):
        if j>=2:
            row,periods=2,[5,10,30]
            colors=px.colors.qualitative.Bold[1:]
        if j%2==0:
            col=1
        else:
            col=2

        for k, period in enumerate(periods):
            if (k==0)&(j<2):
                plot_data=sector_df.groupby(sector_df.index)['AdjustedClose'].mean().rename('Adjusted Close')
            elif j>=2:
                plot_data=sector_df.groupby(sector_df.index)['{}_{}Day'.format(feature,period)].mean().mul(100).rename('{}-day {}'.format(period,name))
            else:
                plot_data=sector_df.groupby(sector_df.index)['{}_{}Day'.format(feature,period)].mean().rename('{}-day {}'.format(period,name))
            fig.add_trace(go.Scatter(x=plot_data.index, y=plot_data, mode='lines',
                                     name=plot_data.name, marker_color=colors[k+1],
                                     line=dict(width=2,dash=(dash[k] if j<2 else 'solid')),
                                     showlegend=(True if (j==0) or (j==2) else False), legendgroup=row,
                                     visible=(False if i != 0 else True)), row=row, col=col)

    visibility=[False]*14*len(price_names.Name.unique())
    for l in range(i*14, i*14+14):
        visibility[l]=True
    button = dict(label = sector,
                  method = "update",
                  args=[{"visible": visibility}])
    buttons.append(button)

fig.update_layout(title='Stock Price Moving Average, Return,<br>and Volatility by Sector',
                  template=temp, yaxis3_ticksuffix='%', yaxis4_ticksuffix='%',
                  legend_title_text='Period', legend_tracegroupgap=250,
                  updatemenus=[dict(active=0, type="dropdown",
                                    buttons=buttons, xanchor='left',
                                    yanchor='bottom', y=1.105, x=.01)],
                  hovermode='x unified', height=800,width=1200, margin=dict(t=150))
fig.show()

price_names=price_features.set_index('Date')
price_names=price_names[price_names.index>'2020-12-31']
price_names=price_names[price_names.index<='2021-12-03']
price_names.fillna(0, inplace=True)

features=['MovingAvg','ExpMovingAvg','Return', 'Volatility']
names=['Average', 'Exp. Moving Average', 'Period', 'Volatility']
buttons=[]

fig = make_subplots(rows=2, cols=2,
                    shared_xaxes=True,
                    vertical_spacing=0.1,
                    subplot_titles=('Adjusted Close Moving Average',
                                    'Exponential Moving Average',
                                    'Stock Return', 'Stock Volatility'))

for i, sector in enumerate(price_names.Name.unique()):

    sector_df=price_names[price_names.Name==sector]
    periods=[5,10,30]
    colors=px.colors.qualitative.Vivid
    dash=['solid','dash', 'longdash', 'dashdot', 'longdashdot']
    row,col=1,1

    for j, (feature, name) in enumerate(zip(features, names)):
        if j>=2:
            row,periods=2,[5,10,30]
            colors=px.colors.qualitative.Bold[1:]
        if j%2==0:
            col=1
        else:
            col=2

        for k, period in enumerate(periods):
            if (k==0)&(j<2):
                plot_data=sector_df.groupby(sector_df.index)['AdjustedClose'].mean().rename('Adjusted Close')
            elif j>=2:
                plot_data=sector_df.groupby(sector_df.index)['{}_{}Day'.format(feature,period)].mean().mul(100).rename('{}-day {}'.format(period,name))
            else:
                plot_data=sector_df.groupby(sector_df.index)['{}_{}Day'.format(feature,period)].mean().rename('{}-day {}'.format(period,name))
            fig.add_trace(go.Scatter(x=plot_data.index, y=plot_data, mode='lines',
                                     name=plot_data.name, marker_color=colors[k+1],
                                     line=dict(width=2,dash=(dash[k] if j<2 else 'solid')),
                                     showlegend=(True if (j==0) or (j==2) else False), legendgroup=row,
                                     visible=(False if i != 0 else True)), row=row, col=col)

    visibility=[False]*14*len(price_names.Name.unique())
    for l in range(i*14, i*14+14):
        visibility[l]=True
    button = dict(label = sector,
                  method = "update",
                  args=[{"visible": visibility}])
    buttons.append(button)

fig.update_layout(title='Stock Price Moving Average, Return,<br>and Volatility by Sector',
                  template=temp, yaxis3_ticksuffix='%', yaxis4_ticksuffix='%',
                  legend_title_text='Period', legend_tracegroupgap=250,
                  updatemenus=[dict(active=0, type="dropdown",
                                    buttons=buttons, xanchor='left',
                                    yanchor='bottom', y=1.105, x=.01)],
                  hovermode='x unified', height=800,width=1200, margin=dict(t=150))
fig.show()

price_features=price_features.assign(incidence="")

cols=['incidence']

price_features.loc[(price_features['Year']==2017)&(price_features['Month']==1),cols]=0
price_features.loc[(price_features['Year']==2017)&(price_features['Month']==2),cols]=0
price_features.loc[(price_features['Year']==2017)&(price_features['Month']==3),cols]=0
price_features.loc[(price_features['Year']==2017)&(price_features['Month']==4),cols]=0
price_features.loc[(price_features['Year']==2017)&(price_features['Month']==5),cols]=1
price_features.loc[(price_features['Year']==2017)&(price_features['Month']==6),cols]=1
price_features.loc[(price_features['Year']==2017)&(price_features['Month']==7),cols]=1
price_features.loc[(price_features['Year']==2017)&(price_features['Month']==8),cols]=1
price_features.loc[(price_features['Year']==2017)&(price_features['Month']==9),cols]=1
price_features.loc[(price_features['Year']==2017)&(price_features['Month']==10),cols]=1
price_features.loc[(price_features['Year']==2017)&(price_features['Month']==11),cols]=1
price_features.loc[(price_features['Year']==2017)&(price_features['Month']==12),cols]=1
price_features.loc[(price_features['Year']==2018)&(price_features['Month']==1),cols]=1
price_features.loc[(price_features['Year']==2018)&(price_features['Month']==2),cols]=1
price_features.loc[(price_features['Year']==2018)&(price_features['Month']==3),cols]=1
price_features.loc[(price_features['Year']==2018)&(price_features['Month']==4),cols]=1
price_features.loc[(price_features['Year']==2018)&(price_features['Month']==5),cols]=0
price_features.loc[(price_features['Year']==2018)&(price_features['Month']==6),cols]=0
price_features.loc[(price_features['Year']==2018)&(price_features['Month']==7),cols]=1
price_features.loc[(price_features['Year']==2018)&(price_features['Month']==8),cols]=1
price_features.loc[(price_features['Year']==2018)&(price_features['Month']==9),cols]=1
price_features.loc[(price_features['Year']==2018)&(price_features['Month']==10),cols]=1
price_features.loc[(price_features['Year']==2018)&(price_features['Month']==11),cols]=1
price_features.loc[(price_features['Year']==2018)&(price_features['Month']==12),cols]=1
price_features.loc[(price_features['Year']==2019)&(price_features['Month']==1),cols]=1
price_features.loc[(price_features['Year']==2019)&(price_features['Month']==2),cols]=1
price_features.loc[(price_features['Year']==2019)&(price_features['Month']==3),cols]=1
price_features.loc[(price_features['Year']==2019)&(price_features['Month']==4),cols]=1
price_features.loc[(price_features['Year']==2019)&(price_features['Month']==5),cols]=1
price_features.loc[(price_features['Year']==2019)&(price_features['Month']==6),cols]=1
price_features.loc[(price_features['Year']==2019)&(price_features['Month']==7),cols]=1
price_features.loc[(price_features['Year']==2019)&(price_features['Month']==8),cols]=1
price_features.loc[(price_features['Year']==2019)&(price_features['Month']==9),cols]=1
price_features.loc[(price_features['Year']==2019)&(price_features['Month']==10),cols]=1
price_features.loc[(price_features['Year']==2019)&(price_features['Month']==11),cols]=1
price_features.loc[(price_features['Year']==2019)&(price_features['Month']==12),cols]=1
price_features.loc[(price_features['Year']==2020)&(price_features['Month']==1),cols]=1
price_features.loc[(price_features['Year']==2020)&(price_features['Month']==2),cols]=1
price_features.loc[(price_features['Year']==2020)&(price_features['Month']==3),cols]=1
price_features.loc[(price_features['Year']==2020)&(price_features['Month']==4),cols]=0
price_features.loc[(price_features['Year']==2020)&(price_features['Month']==5),cols]=0
price_features.loc[(price_features['Year']==2020)&(price_features['Month']==6),cols]=0
price_features.loc[(price_features['Year']==2020)&(price_features['Month']==7),cols]=0
price_features.loc[(price_features['Year']==2020)&(price_features['Month']==8),cols]=0
price_features.loc[(price_features['Year']==2020)&(price_features['Month']==9),cols]=1
price_features.loc[(price_features['Year']==2020)&(price_features['Month']==10),cols]=1
price_features.loc[(price_features['Year']==2020)&(price_features['Month']==11),cols]=1
price_features.loc[(price_features['Year']==2020)&(price_features['Month']==12),cols]=1
price_features.loc[(price_features['Year']==2021)&(price_features['Month']==1),cols]=1
price_features.loc[(price_features['Year']==2021)&(price_features['Month']==2),cols]=1
price_features.loc[(price_features['Year']==2021)&(price_features['Month']==3),cols]=0
price_features.loc[(price_features['Year']==2021)&(price_features['Month']==4),cols]=0
price_features.loc[(price_features['Year']==2021)&(price_features['Month']==5),cols]=0
price_features.loc[(price_features['Year']==2021)&(price_features['Month']==6),cols]=0
price_features.loc[(price_features['Year']==2021)&(price_features['Month']==7),cols]=0
price_features.loc[(price_features['Year']==2021)&(price_features['Month']==8),cols]=0
price_features.loc[(price_features['Year']==2021)&(price_features['Month']==9),cols]=0
price_features.loc[(price_features['Year']==2021)&(price_features['Month']==10),cols]=1
price_features.loc[(price_features['Year']==2021)&(price_features['Month']==11),cols]=1
price_features.loc[(price_features['Year']==2021)&(price_features['Month']==12),cols]=1

price_features['incidence']=pd.to_numeric(price_features['incidence'],errors="coerce")

price_features['incidence']

price_features

price_features=price_features.set_index('Date')

price_features

def calc_spread_return_sharpe(df: pd.DataFrame, portfolio_size: int = 14, toprank_weight_ratio: float = 2) -> float:
    """
    Args:
        df (pd.DataFrame): predicted results
        portfolio_size (int): # of equities to buy/sell
        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.
    Returns:
        (float): sharpe ratio
    """
    def _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):
        """
        Args:
            df (pd.DataFrame): predicted results
            portfolio_size (int): # of equities to buy/sell
            toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.
        Returns:
            (float): spread return
        """
        assert df['Rank'].min() == 0
        assert df['Rank'].max() == len(df['Rank']) - 1
        weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)
        purchase = (df.sort_values(by='Rank')['Target'][:portfolio_size] * weights).sum() / weights.mean()
        short = (df.sort_values(by='Rank', ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()
        return purchase - short

    buf = df.groupby('Date').apply(_calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)
    sharpe_ratio = buf.mean() / buf.std()
    return sharpe_ratio

ts_fold = TimeSeriesSplit(n_splits=10, gap=500)
prices=price_features.dropna().sort_values(['Date','SecuritiesCode'])
y=prices['Target'].to_numpy()
X=prices.drop(['Target'],axis=1)

feat_importance=pd.DataFrame()
sharpe_ratio=[]

for fold, (train_idx, val_idx) in enumerate(ts_fold.split(X, y)):

    print("\n========================== Fold {} ==========================".format(fold+1))
    X_train, y_train = X.iloc[train_idx,:], y[train_idx]
    X_valid, y_val = X.iloc[val_idx,:], y[val_idx]

    print("Train Date range: {} to {}".format(X_train.Date.min(),X_train.Date.max()))
    print("Valid Date range: {} to {}".format(X_valid.Date.min(),X_valid.Date.max()))

    X_train.drop(['Date','SecuritiesCode','Name','SectorName'], axis=1, inplace=True)
    X_val=X_valid[X_valid.columns[~X_valid.columns.isin(['Date','SecuritiesCode','Name','SectorName'])]]
    val_dates=X_valid.Date.unique()[1:-1]
    print("\nTrain Shape: {} {}, Valid Shape: {} {}".format(X_train.shape, y_train.shape, X_val.shape, y_val.shape))

    params = {'n_estimators': 500,
              'num_leaves' : 100,
              'learning_rate': 0.1,
              'colsample_bytree': 0.9,
              'subsample': 0.8,
              'reg_alpha': 0.4,
              'metric': 'mae',
              'random_state': 21}

    gbm = LGBMRegressor(**params).fit(X_train, y_train,
                                      eval_set=[(X_train, y_train), (X_val, y_val)],
                                      verbose=300,
                                      eval_metric=['mae','mse'])
    y_pred = gbm.predict(X_val)
    mse = (mean_squared_error(y_val, y_pred))
    mae = mean_absolute_error(y_val, y_pred)
    feat_importance["Importance_Fold"+str(fold)]=gbm.feature_importances_
    feat_importance.set_index(X_train.columns, inplace=True)

    rank=[]
    X_val_df=X_valid[X_valid.Date.isin(val_dates)]
    for i in X_val_df.Date.unique():
        temp_df = X_val_df[X_val_df.Date == i].drop(['Date','SecuritiesCode','Name','SectorName'],axis=1)
        temp_df["pred"] = gbm.predict(temp_df)
        temp_df["Rank"] = (temp_df["pred"].rank(method="first", ascending=False)-1).astype(int)
        rank.append(temp_df["Rank"].values)

    stock_rank=pd.Series([x for y in rank for x in y], name="Rank")
    df=pd.concat([X_val_df.reset_index(drop=True),stock_rank,
                  prices[prices.Date.isin(val_dates)]['Target'].reset_index(drop=True)], axis=1)
    sharpe=calc_spread_return_sharpe(df)
    sharpe_ratio.append(sharpe)
    print("Valid Sharpe: {}, MSE: {}, MAE: {}".format(sharpe,mse,mae))

    del X_train, y_train,  X_val, y_val
    gc.collect()

print("\nAverage cross-validation Sharpe Ratio: {:.4f}, standard deviation = {:.2f}.".format(np.mean(sharpe_ratio),np.std(sharpe_ratio)))

stock_rank.head(14)

feat_importance['avg'] = feat_importance.mean(axis=1)
feat_importance = feat_importance.sort_values(by='avg',ascending=True)
pal=sns.color_palette("plasma_r", 29).as_hex()[2:]

fig=go.Figure()
for i in range(len(feat_importance.index)):
    fig.add_shape(dict(type="line", y0=i, y1=i, x0=0, x1=feat_importance['avg'][i],
                       line_color=pal[::-1][i],opacity=0.7,line_width=4))
fig.add_trace(go.Scatter(x=feat_importance['avg'], y=feat_importance.index, mode='markers',
                         marker_color=pal[::-1], marker_size=8,
                         hovertemplate='%{y} Importance = %{x:.0f}<extra></extra>'))
fig.update_layout(template=temp,title='Overall Feature Importance',
                  xaxis=dict(title='Average Importance',zeroline=False),
                  yaxis_showgrid=False, margin=dict(l=120,t=80),
                  height=700, width=800)
fig.show()

cols_fin=feat_importance.avg.nlargest(3).index.tolist()
cols_fin.extend(('Open','High','Low'))
X_train=prices[cols_fin]
y_train=prices['Target']
gbm = LGBMRegressor(**params).fit(X_train, y_train)

gbm

import pydotplus

import math
import yfinance as yf
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from keras.layers import LSTM
from keras.layers import Dense, Dropout

from keras.wrappers.scikit_learn import KerasRegressor   # Regression prediction by keras
from sklearn.model_selection import GridSearchCV
from keras.wrappers.scikit_learn import KerasClassifier
from keras import optimizers
from tensorflow.keras.constraints import MaxNorm
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.utils import plot_model

colmn = list(price_features.columns.values)
print(colmn)

"""# หุ้น1 30 day"""

features=['Open', 'High', 'Low','Close','incidence']
prices2 = price_features.query("SecuritiesCode==5015")

prices2

test_split = math.ceil(len(prices2)*0.8)

df_for_training = prices2[features]
print(df_for_training.shape)
df_for_training = df_for_training.dropna(how='any')
print(df_for_training.shape)

df_for_testing = prices2[features]
df_for_testing = df_for_testing.dropna(how='any')
print(df_for_testing.shape)

scaler = MinMaxScaler(feature_range=(0,1))   # Zoom data
df_for_training_scaled = scaler.fit_transform(df_for_training)
df_for_training_scaled=df_for_training_scaled[0:test_split,:]
df_for_testing_scaled = scaler.transform(df_for_testing)
df_for_testing_scaled=df_for_testing_scaled[test_split-30:,:]

# createXY
def createXY(dataset,n_past):
    dataX = []
    dataY = []
    for i in range(n_past, len(dataset)):
            dataX.append(dataset[i - n_past:i, 0:dataset.shape[1]])
            dataY.append(dataset[i,0])
    return np.array(dataX),np.array(dataY)




# Generate data
trainX,trainY=createXY(df_for_training_scaled,30)
# trainX.shape
testX,testY=createXY(df_for_testing_scaled,30)

print(testX.shape)

sgd=optimizers.SGD()

grid_model = keras.Sequential()
grid_model.add(LSTM(48,return_sequences='True',activation='relu',input_shape=(30,5)))
grid_model.add((Dropout(0.2)))
grid_model.add(LSTM(24))
grid_model.add((Dropout(0.2)))
grid_model.add((Dense(1)))
grid_model.summary()

tf.keras.utils.plot_model(model=grid_model,show_shapes=True)

grid_model.compile(loss='mean_squared_error', optimizer='adam')
history = grid_model.fit(trainX,trainY,epochs=200,batch_size=14,validation_data=(testX,testY))

from matplotlib import pyplot as plt
plt.plot(history.history['loss'],label='train')
plt.plot(history.history['val_loss'],label='test')
plt.legend()
plt.show()

prediction=grid_model.predict(testX)

prediction_copies_array = np.repeat(prediction,5, axis=-1)

pred=scaler.inverse_transform(np.reshape(prediction_copies_array,(len(prediction),5)))[:,0]  # Reverse change, and the standardized data is converted into the original data

# Real data
original_copies_array = np.repeat(testY,5, axis=-1)

# original_copies_array.shape

original=scaler.inverse_transform(np.reshape(original_copies_array,(len(testY),5)))[:,0]
rmse = np.sqrt(np.mean(pred-original)**2)
rmse

prediction1=grid_model.predict(trainX)
prediction_copies_array1 = np.repeat(prediction1,5, axis=-1)
pred1=scaler.inverse_transform(np.reshape(prediction_copies_array1,(len(prediction1),5)))[:,0]
# Real data
original_copies_array1 = np.repeat(trainY,5, axis=-1)

# original_copies_array.shape

original1=scaler.inverse_transform(np.reshape(original_copies_array1,(len(trainY),5)))[:,0]
rmse = np.sqrt(np.mean(pred1-original1)**2)
rmse

original1

plt.plot(original, color = 'red', label = 'Real  Stock Price')
plt.plot(pred, color = 'blue', label = 'Predicted  Stock Price')
plt.title(' Stock Price Prediction')
plt.xlabel('Date')
plt.ylabel(' Stock Price')
plt.legend()
plt.show()

data = prices2.filter(['Close'])
csfont = {'fontname':'Times New Roman'}
train=data[test_split:]
validate = data[test_split:]
validate['prediction']=pred
plt.figure(figsize=(16,8))
plt.xlabel('Date',**csfont,fontsize=18,labelpad=15)
plt.ylabel('Close price',**csfont,fontsize=18,labelpad=15)
plt.plot(validate[['Close']], color='Red')
plt.plot(validate[['prediction']], color='Blue')
plt.legend(['Val','Prediction'],fontsize=10)
plt.show()

train

features=['Open', 'High', 'Low','Close','incidence']
prices2 = price_features.query("SecuritiesCode==5015")

test_split = math.ceil(len(prices2)*0.8)
df_for_training = prices2[features]
print(df_for_training.shape)
df_for_training = df_for_training.dropna(how='any')
print(df_for_training.shape)
df_for_testing = prices2[features]
df_for_testing = df_for_testing.dropna(how='any')
print(df_for_testing.shape)

scaler = MinMaxScaler(feature_range=(0,1))   # Zoom data
df_for_training_scaled = scaler.fit_transform(df_for_training)
df_for_training_scaled=df_for_training_scaled[0:test_split,:]
df_for_testing_scaled = scaler.transform(df_for_testing)
df_for_testing_scaled=df_for_testing_scaled[test_split-15:,:]

# createXY
def createXY(dataset,n_past):
    dataX = []
    dataY = []
    for i in range(n_past, len(dataset)):
            dataX.append(dataset[i - 15:i, 0:dataset.shape[1]])
            dataY.append(dataset[i,0])
    return np.array(dataX),np.array(dataY)
# Generate data
trainX,trainY=createXY(df_for_training_scaled,15)
# trainX.shape
testX,testY=createXY(df_for_testing_scaled,15)

print(trainX.shape)

grid_model = keras.Sequential()
grid_model.add(LSTM(48,return_sequences='True',activation='relu',input_shape=(15,5)))
grid_model.add((Dropout(0.2)))
grid_model.add(LSTM(24))
grid_model.add((Dropout(0.2)))
grid_model.add((Dense(1)))
grid_model.summary()

tf.keras.utils.plot_model(model=grid_model,show_shapes=True)

grid_model.compile(loss='mean_squared_error', optimizer='adam')
history = grid_model.fit(trainX,trainY,epochs=200,batch_size=14,validation_data=(testX,testY))

from matplotlib import pyplot as plt
plt.plot(history.history['loss'],label='train')
plt.plot(history.history['val_loss'],label='test')
plt.legend()
plt.show()

prediction=grid_model.predict(testX)

prediction_copies_array = np.repeat(prediction,5, axis=-1)

pred=scaler.inverse_transform(np.reshape(prediction_copies_array,(len(prediction),5)))[:,0]
# Real data
original_copies_array = np.repeat(testY,5, axis=-1)

# original_copies_array.shape

original=scaler.inverse_transform(np.reshape(original_copies_array,(len(testY),5)))[:,0]
original=scaler.inverse_transform(np.reshape(original_copies_array,(len(testY),5)))[:,0]
rmse = np.sqrt(np.mean(pred-original)**2)
rmse

prediction1=grid_model.predict(trainX)
prediction_copies_array1 = np.repeat(prediction1,5, axis=-1)
pred1=scaler.inverse_transform(np.reshape(prediction_copies_array1,(len(prediction1),5)))[:,0]
# Real data
original_copies_array1 = np.repeat(trainY,5, axis=-1)

# original_copies_array.shape

original1=scaler.inverse_transform(np.reshape(original_copies_array1,(len(trainY),5)))[:,0]
rmse = np.sqrt(np.mean(pred1-original1)**2)
rmse

plt.plot(original, color = 'red', label = 'Real  Stock Price')
plt.plot(pred, color = 'blue', label = 'Predicted  Stock Price')
plt.title(' Stock Price Prediction')
plt.xlabel('Time')
plt.ylabel(' Stock Price')
plt.legend()
plt.show()

data = prices2.filter(['Close'])
csfont = {'fontname':'Times New Roman'}
train=data[test_split:]
validate = data[test_split:]
validate['prediction']=pred
plt.figure(figsize=(16,8))
plt.xlabel('Date',**csfont,fontsize=18,labelpad=15)
plt.ylabel('Close price',**csfont,fontsize=18,labelpad=15)
plt.plot(validate[['Close']], color='Red')
plt.plot(validate[['prediction']], color='Blue')
plt.legend(['Val','Prediction'],fontsize=10)
plt.show()

features=['Open', 'High', 'Low','Close','incidence']
prices2 = price_features.query("SecuritiesCode==5015")

test_split = math.ceil(len(prices2)*0.8)
df_for_training = prices2[features]
print(df_for_training.shape)
df_for_training = df_for_training.dropna(how='any')
print(df_for_training.shape)
df_for_testing = prices2[features]
df_for_testing = df_for_testing.dropna(how='any')
print(df_for_testing.shape)

scaler = MinMaxScaler(feature_range=(0,1))   # Zoom data
df_for_training_scaled = scaler.fit_transform(df_for_training)
df_for_training_scaled=df_for_training_scaled[0:test_split,:]
df_for_testing_scaled = scaler.transform(df_for_testing)
df_for_testing_scaled=df_for_testing_scaled[test_split-5:,:]

# createXY
def createXY(dataset,n_past):
    dataX = []
    dataY = []
    for i in range(n_past, len(dataset)):
            dataX.append(dataset[i - 5:i, 0:dataset.shape[1]])
            dataY.append(dataset[i, 0])
    return np.array(dataX),np.array(dataY)
# Generate data
trainX,trainY=createXY(df_for_training_scaled,5)
# trainX.shape
testX,testY=createXY(df_for_testing_scaled,5)

print(trainX.shape)

grid_model = keras.Sequential()
grid_model.add(LSTM(48,return_sequences='True',activation='relu',input_shape=(5,5)))
grid_model.add((Dropout(0.2)))
grid_model.add(LSTM(24))
grid_model.add((Dropout(0.2)))
grid_model.add((Dense(1)))
grid_model.summary()

tf.keras.utils.plot_model(model=grid_model,show_shapes=True)

grid_model.compile(loss='mean_squared_error', optimizer='adam')

history = grid_model.fit(trainX,trainY,epochs=200,batch_size=2,validation_data=(testX,testY))

from matplotlib import pyplot as plt
plt.plot(history.history['loss'],label='train')
plt.plot(history.history['val_loss'],label='test')
plt.legend()
plt.show()

prediction=grid_model.predict(testX)

prediction_copies_array = np.repeat(prediction,5, axis=-1)

pred=scaler.inverse_transform(np.reshape(prediction_copies_array,(len(prediction),5)))[:,0]
# Real data
original_copies_array = np.repeat(testY,5, axis=-1)

# original_copies_array.shape

original=scaler.inverse_transform(np.reshape(original_copies_array,(len(testY),5)))[:,0]
rmse = np.sqrt(np.mean(pred-original)**2)
rmse

prediction1=grid_model.predict(trainX)
prediction_copies_array1 = np.repeat(prediction1,5, axis=-1)
pred1=scaler.inverse_transform(np.reshape(prediction_copies_array1,(len(prediction1),5)))[:,0]
# Real data
original_copies_array1 = np.repeat(trainY,5, axis=-1)

# original_copies_array.shape

original1=scaler.inverse_transform(np.reshape(original_copies_array1,(len(trainY),5)))[:,0]
rmse = np.sqrt(np.mean(pred1-original1)**2)
rmse

plt.plot(original, color = 'red', label = 'Real  Stock Price')
plt.plot(pred, color = 'blue', label = 'Predicted  Stock Price')
plt.title(' Stock Price Prediction')
plt.xlabel('Time')
plt.ylabel(' Stock Price')
plt.legend()
plt.show()

data = prices2.filter(['Close'])
csfont = {'fontname':'Times New Roman'}
train=data[test_split:]
validate = data[test_split:]
validate['prediction']=pred
plt.figure(figsize=(16,8))
plt.xlabel('Date',**csfont,fontsize=18,labelpad=15)
plt.ylabel('Close price',**csfont,fontsize=18,labelpad=15)
plt.plot(validate[['Close']], color='Red')
plt.plot(validate[['prediction']], color='Blue')
plt.legend(['Val','Prediction'],fontsize=10)
plt.show()

"""# หุ้น2

"""

features=['Open', 'High', 'Low','Close','incidence']
prices3 = price_features.query("SecuritiesCode==5013")

test_split = math.ceil(len(prices2)*0.8)
df_for_training = prices3[features]
print(df_for_training.shape)
df_for_training = df_for_training.dropna(how='any')
print(df_for_training.shape)
df_for_testing = prices3[features]
df_for_testing = df_for_testing.dropna(how='any')
print(df_for_testing.shape)

scaler = MinMaxScaler(feature_range=(0,1))   # Zoom data
df_for_training_scaled = scaler.fit_transform(df_for_training)
df_for_training_scaled=df_for_training_scaled[0:test_split,:]
df_for_testing_scaled = scaler.transform(df_for_testing)
df_for_testing_scaled=df_for_testing_scaled[test_split-5:,:]

# createXY
def createXY(dataset,n_past):
    dataX = []
    dataY = []
    for i in range(n_past, len(dataset)):
            dataX.append(dataset[i - 5:i, 0:dataset.shape[1]])
            dataY.append(dataset[i,0])
    return np.array(dataX),np.array(dataY)
# Generate data
trainX,trainY=createXY(df_for_training_scaled,5)
# trainX.shape
testX,testY=createXY(df_for_testing_scaled,5)

grid_model = keras.Sequential()
grid_model.add(LSTM(25,return_sequences='True',activation='relu',input_shape=(5,5)))
grid_model.add((Dropout(0.2)))
grid_model.add(LSTM(12))
grid_model.add((Dropout(0.2)))
grid_model.add((Dense(1)))
grid_model.summary()

grid_model.compile(loss='mean_squared_error', optimizer='adam')

tf.keras.utils.plot_model(model=grid_model,show_shapes=True)

history = grid_model.fit(trainX,trainY,epochs=200,batch_size=4,validation_data=(testX,testY))

from matplotlib import pyplot as plt
plt.plot(history.history['loss'],label='train')
plt.plot(history.history['val_loss'],label='test')
plt.legend()
plt.show()

prediction=grid_model.predict(testX)

prediction_copies_array = np.repeat(prediction,5, axis=-1)

pred=scaler.inverse_transform(np.reshape(prediction_copies_array,(len(prediction),5)))[:,0]
# Real data
original_copies_array = np.repeat(testY,5, axis=-1)

# original_copies_array.shape

original=scaler.inverse_transform(np.reshape(original_copies_array,(len(testY),5)))[:,0]
rmse = np.sqrt(np.mean(pred-original)**2)
rmse

prediction1=grid_model.predict(trainX)
prediction_copies_array1 = np.repeat(prediction1,5, axis=-1)
pred1=scaler.inverse_transform(np.reshape(prediction_copies_array1,(len(prediction1),5)))[:,0]
# Real data
original_copies_array1 = np.repeat(trainY,5, axis=-1)

# original_copies_array.shape

original1=scaler.inverse_transform(np.reshape(original_copies_array1,(len(trainY),5)))[:,0]
rmse = np.sqrt(np.mean(pred1-original1)**2)
rmse

plt.plot(original, color = 'red', label = 'Real  Stock Price')
plt.plot(pred, color = 'blue', label = 'Predicted  Stock Price')
plt.title(' Stock Price Prediction')
plt.xlabel('Time')
plt.ylabel(' Stock Price')
plt.legend()
plt.show()

data = prices3.filter(['Close'])
csfont = {'fontname':'Times New Roman'}
train=data[test_split:]
validate = data[test_split:]
validate['prediction']=pred
plt.figure(figsize=(16,8))
plt.xlabel('Date',**csfont,fontsize=18,labelpad=15)
plt.ylabel('Close price',**csfont,fontsize=18,labelpad=15)
plt.plot(validate[['Close']], color='Red')
plt.plot(validate[['prediction']], color='Blue')
plt.legend(['Val','Prediction'],fontsize=10)
plt.show()

test_split = math.ceil(len(prices2)*0.8)
df_for_training = prices3[features]
print(df_for_training.shape)
df_for_training = df_for_training.dropna(how='any')
print(df_for_training.shape)
df_for_testing = prices3[features]
df_for_testing = df_for_testing.dropna(how='any')
print(df_for_testing.shape)

scaler = MinMaxScaler(feature_range=(0,1))   # Zoom data
df_for_training_scaled = scaler.fit_transform(df_for_training)
df_for_training_scaled=df_for_training_scaled[0:test_split,:]
df_for_testing_scaled = scaler.transform(df_for_testing)
df_for_testing_scaled=df_for_testing_scaled[test_split-15:,:]

# createXY
def createXY(dataset,n_past):
    dataX = []
    dataY = []
    for i in range(n_past, len(dataset)):
            dataX.append(dataset[i - 15:i, 0:dataset.shape[1]])
            dataY.append(dataset[i,0])
    return np.array(dataX),np.array(dataY)
# Generate data
trainX,trainY=createXY(df_for_training_scaled,15)
# trainX.shape
testX,testY=createXY(df_for_testing_scaled,15)

grid_model = keras.Sequential()
grid_model.add(LSTM(50,return_sequences='True',activation='relu',input_shape=(15,5)))
grid_model.add((Dropout(0.2)))
grid_model.add(LSTM(25))
grid_model.add((Dropout(0.2)))
grid_model.add((Dense(1)))
grid_model.summary()

grid_model.compile(loss='mean_squared_error', optimizer='adam')

tf.keras.utils.plot_model(model=grid_model,show_shapes=True)

history = grid_model.fit(trainX,trainY,epochs=200,batch_size=16,validation_data=(testX,testY))

from matplotlib import pyplot as plt
plt.plot(history.history['loss'],label='train')
plt.plot(history.history['val_loss'],label='test')
plt.legend()
plt.show()

prediction=grid_model.predict(testX)

prediction_copies_array = np.repeat(prediction,5, axis=-1)

pred=scaler.inverse_transform(np.reshape(prediction_copies_array,(len(prediction),5)))[:,0]
# Real data
original_copies_array = np.repeat(testY,5, axis=-1)

# original_copies_array.shape

original=scaler.inverse_transform(np.reshape(original_copies_array,(len(testY),5)))[:,0]
rmse = np.sqrt(np.mean(pred-original)**2)
rmse

prediction1=grid_model.predict(trainX)
prediction_copies_array1 = np.repeat(prediction1,5, axis=-1)
pred1=scaler.inverse_transform(np.reshape(prediction_copies_array1,(len(prediction1),5)))[:,0]
# Real data
original_copies_array1 = np.repeat(trainY,5, axis=-1)

# original_copies_array.shape

original1=scaler.inverse_transform(np.reshape(original_copies_array1,(len(trainY),5)))[:,0]
rmse = np.sqrt(np.mean(pred1-original1)**2)
rmse

plt.plot(original, color = 'red', label = 'Real  Stock Price')
plt.plot(pred, color = 'blue', label = 'Predicted  Stock Price')
plt.title(' Stock Price Prediction')
plt.xlabel('Time')
plt.ylabel(' Stock Price')
plt.legend()
plt.show()

data = prices3.filter(['Close'])
csfont = {'fontname':'Times New Roman'}
train=data[test_split:]
validate = data[test_split:]
validate['prediction']=pred
plt.figure(figsize=(16,8))
plt.xlabel('Date',**csfont,fontsize=18,labelpad=15)
plt.ylabel('Close price',**csfont,fontsize=18,labelpad=15)
plt.plot(validate[['Close']], color='Red')
plt.plot(validate[['prediction']], color='Blue')
plt.legend(['Val','Prediction'],fontsize=10)
plt.show()

test_split = math.ceil(len(prices2)*0.8)
df_for_training = prices3[features]
print(df_for_training.shape)
df_for_training = df_for_training.dropna(how='any')
print(df_for_training.shape)
df_for_testing = prices3[features]
df_for_testing = df_for_testing.dropna(how='any')
print(df_for_testing.shape)

scaler = MinMaxScaler(feature_range=(0,1))   # Zoom data
df_for_training_scaled = scaler.fit_transform(df_for_training)
df_for_training_scaled=df_for_training_scaled[0:test_split,:]
df_for_testing_scaled = scaler.transform(df_for_testing)
df_for_testing_scaled=df_for_testing_scaled[test_split-30:,:]

# createXY
def createXY(dataset,n_past):
    dataX = []
    dataY = []
    for i in range(n_past, len(dataset)):
            dataX.append(dataset[i - 30:i, 0:dataset.shape[1]])
            dataY.append(dataset[i,0])
    return np.array(dataX),np.array(dataY)
# Generate data
trainX,trainY=createXY(df_for_training_scaled,30)
# trainX.shape
testX,testY=createXY(df_for_testing_scaled,30)

grid_model = keras.Sequential()
grid_model.add(LSTM(50,return_sequences='True',activation='relu',input_shape=(30,5)))
grid_model.add((Dropout(0.2)))
grid_model.add(LSTM(30))
grid_model.add((Dropout(0.2)))
grid_model.add((Dense(1)))
grid_model.summary()

grid_model.compile(loss='mean_squared_error', optimizer='adam')

tf.keras.utils.plot_model(model=grid_model,show_shapes=True)

history = grid_model.fit(trainX,trainY,epochs=200,batch_size=16,validation_data=(testX,testY))

from matplotlib import pyplot as plt
plt.plot(history.history['loss'],label='train')
plt.plot(history.history['val_loss'],label='test6')
plt.legend()
plt.show()

prediction=grid_model.predict(testX)

prediction_copies_array = np.repeat(prediction,5, axis=-1)

pred=scaler.inverse_transform(np.reshape(prediction_copies_array,(len(prediction),5)))[:,0]

# Real data
original_copies_array = np.repeat(testY,5, axis=-1)

# original_copies_array.shape

original=scaler.inverse_transform(np.reshape(original_copies_array,(len(testY),5)))[:,0]


rmse = np.sqrt(np.mean(pred-original)**2)
rmse

prediction1=grid_model.predict(trainX)
prediction_copies_array1 = np.repeat(prediction1,5, axis=-1)
pred1=scaler.inverse_transform(np.reshape(prediction_copies_array1,(len(prediction1),5)))[:,0]
# Real data
original_copies_array1 = np.repeat(trainY,5, axis=-1)

# original_copies_array.shape

original1=scaler.inverse_transform(np.reshape(original_copies_array1,(len(trainY),5)))[:,0]
rmse = np.sqrt(np.mean(pred1-original1)**2)
rmse

plt.plot(original, color = 'red', label = 'Real  Stock Price')
plt.plot(pred, color = 'blue', label = 'Predicted  Stock Price')
plt.title(' Stock Price Prediction')
plt.xlabel('Time')
plt.ylabel(' Stock Price')
plt.legend()
plt.show()

data = prices3.filter(['Close'])
csfont = {'fontname':'Times New Roman'}
train=data[test_split:]
validate = data[test_split:]
validate['prediction']=pred
plt.figure(figsize=(16,8))
plt.xlabel('Date',**csfont,fontsize=18,labelpad=15)
plt.ylabel('Close price',**csfont,fontsize=18,labelpad=15)
plt.plot(validate[['Close']], color='Red')
plt.plot(validate[['prediction']], color='Blue')
plt.legend(['Val','Prediction'],fontsize=10)
plt.show()

features=['Open', 'High', 'Low','Close','incidence']
prices4 = price_features.query("SecuritiesCode==5011")

test_split = math.ceil(len(prices2)*0.8)
df_for_training = prices4[features]
print(df_for_training.shape)
df_for_training = df_for_training.dropna(how='any')
print(df_for_training.shape)
df_for_testing = prices4[features]
df_for_testing = df_for_testing.dropna(how='any')
print(df_for_testing.shape)

scaler = MinMaxScaler(feature_range=(0,1))   # Zoom data
df_for_training_scaled = scaler.fit_transform(df_for_training)
df_for_training_scaled=df_for_training_scaled[0:test_split,:]
df_for_testing_scaled = scaler.transform(df_for_testing)
df_for_testing_scaled=df_for_testing_scaled[test_split-30:,:]

# createXY
def createXY(dataset,n_past):
    dataX = []
    dataY = []
    for i in range(n_past, len(dataset)):
            dataX.append(dataset[i - 30:i, 0:dataset.shape[1]])
            dataY.append(dataset[i,0])
    return np.array(dataX),np.array(dataY)
# Generate data
trainX,trainY=createXY(df_for_training_scaled,30)
# trainX.shape
testX,testY=createXY(df_for_testing_scaled,30)

grid_model = keras.Sequential()
grid_model.add(LSTM(30,return_sequences='True',activation='relu',input_shape=(30,5)))
grid_model.add((Dropout(0.2)))
grid_model.add(LSTM(30))
grid_model.add((Dropout(0.2)))
grid_model.add((Dense(1)))
grid_model.summary()

tf.keras.utils.plot_model(model=grid_model,show_shapes=True)

grid_model.compile(loss='mean_squared_error', optimizer='adam')
history = grid_model.fit(trainX,trainY,epochs=200,batch_size=64,validation_data=(testX,testY))

from matplotlib import pyplot as plt
plt.plot(history.history['loss'],label='train')
plt.plot(history.history['val_loss'],label='test6')
plt.legend()
plt.show()
prediction=grid_model.predict(testX)

prediction_copies_array = np.repeat(prediction,5, axis=-1)

pred=scaler.inverse_transform(np.reshape(prediction_copies_array,(len(prediction),5)))[:,0]

# Real data
original_copies_array = np.repeat(testY,5, axis=-1)

# original_copies_array.shape

original=scaler.inverse_transform(np.reshape(original_copies_array,(len(testY),5)))[:,0]
rmse = np.sqrt(np.mean(pred-original)**2)
rmse

prediction1=grid_model.predict(trainX)
prediction_copies_array1 = np.repeat(prediction1,5, axis=-1)
pred1=scaler.inverse_transform(np.reshape(prediction_copies_array1,(len(prediction1),5)))[:,0]
# Real data
original_copies_array1 = np.repeat(trainY,5, axis=-1)

# original_copies_array.shape

original1=scaler.inverse_transform(np.reshape(original_copies_array1,(len(trainY),5)))[:,0]
rmse = np.sqrt(np.mean(pred1-original1)**2)
rmse

plt.plot(original, color = 'red', label = 'Real  Stock Price')
plt.plot(pred, color = 'blue', label = 'Predicted  Stock Price')
plt.title(' Stock Price Prediction')
plt.xlabel('Time')
plt.ylabel(' Stock Price')
plt.legend()
plt.show()

data = prices4.filter(['Close'])
csfont = {'fontname':'Times New Roman'}
train=data[test_split:]
validate = data[test_split:]
validate['prediction']=pred
plt.figure(figsize=(16,8))
plt.xlabel('Date',**csfont,fontsize=18,labelpad=15)
plt.ylabel('Close price',**csfont,fontsize=18,labelpad=15)
plt.plot(validate[['Close']], color='Red')
plt.plot(validate[['prediction']], color='Blue')
plt.legend(['Val','Prediction'],fontsize=10)
plt.show()

test_split = math.ceil(len(prices2)*0.8)
df_for_training = prices4[features]
print(df_for_training.shape)
df_for_training = df_for_training.dropna(how='any')
print(df_for_training.shape)
df_for_testing = prices4[features]
df_for_testing = df_for_testing.dropna(how='any')
print(df_for_testing.shape)

scaler = MinMaxScaler(feature_range=(0,1))   # Zoom data
df_for_training_scaled = scaler.fit_transform(df_for_training)
df_for_training_scaled=df_for_training_scaled[0:test_split,:]
df_for_testing_scaled = scaler.transform(df_for_testing)
df_for_testing_scaled=df_for_testing_scaled[test_split-15:,:]

# createXY
def createXY(dataset,n_past):
    dataX = []
    dataY = []
    for i in range(n_past, len(dataset)):
            dataX.append(dataset[i - 15:i, 0:dataset.shape[1]])
            dataY.append(dataset[i,0])
    return np.array(dataX),np.array(dataY)
# Generate data
trainX,trainY=createXY(df_for_training_scaled,15)
# trainX.shape
testX,testY=createXY(df_for_testing_scaled,15)

grid_model = keras.Sequential()
grid_model.add(LSTM(30,return_sequences='True',activation='relu',input_shape=(15,5)))
grid_model.add((Dropout(0.2)))
grid_model.add(LSTM(30))
grid_model.add((Dropout(0.2)))
grid_model.add((Dense(1)))
grid_model.summary()

tf.keras.utils.plot_model(model=grid_model,show_shapes=True)

grid_model.compile(loss='mean_squared_error', optimizer='adam')
history = grid_model.fit(trainX,trainY,epochs=200,batch_size=16,validation_data=(testX,testY))

from matplotlib import pyplot as plt
plt.plot(history.history['loss'],label='train')
plt.plot(history.history['val_loss'],label='test6')
plt.legend()
plt.show()
prediction=grid_model.predict(testX)

prediction_copies_array = np.repeat(prediction,5, axis=-1)

pred=scaler.inverse_transform(np.reshape(prediction_copies_array,(len(prediction),5)))[:,0]  # Reverse change, and the standardized data is converted into the original data

# Real data
original_copies_array = np.repeat(testY,5, axis=-1)

# original_copies_array.shape

original=scaler.inverse_transform(np.reshape(original_copies_array,(len(testY),5)))[:,0]
rmse = np.sqrt(np.mean(pred-original)**2)
rmse

prediction1=grid_model.predict(trainX)
prediction_copies_array1 = np.repeat(prediction1,5, axis=-1)
pred1=scaler.inverse_transform(np.reshape(prediction_copies_array1,(len(prediction1),5)))[:,0]
# Real data
original_copies_array1 = np.repeat(trainY,5, axis=-1)

# original_copies_array.shape

original1=scaler.inverse_transform(np.reshape(original_copies_array1,(len(trainY),5)))[:,0]
rmse = np.sqrt(np.mean(pred1-original1)**2)
rmse

plt.plot(original, color = 'red', label = 'Real  Stock Price')
plt.plot(pred, color = 'blue', label = 'Predicted  Stock Price')
plt.title(' Stock Price Prediction')
plt.xlabel('Time')
plt.ylabel(' Stock Price')
plt.legend()
plt.show()

data = prices4.filter(['Close'])
csfont = {'fontname':'Times New Roman'}
train=data[test_split:]
validate = data[test_split:]
validate['prediction']=pred
plt.figure(figsize=(16,8))
plt.xlabel('Date',**csfont,fontsize=18,labelpad=15)
plt.ylabel('Close price',**csfont,fontsize=18,labelpad=15)
plt.plot(validate[['Close']], color='Red')
plt.plot(validate[['prediction']], color='Blue')
plt.legend(['Val','Prediction'],fontsize=10)
plt.show()

test_split = math.ceil(len(prices2)*0.8)
df_for_training = prices4[features]
print(df_for_training.shape)
df_for_training = df_for_training.dropna(how='any')
print(df_for_training.shape)
df_for_testing = prices4[features]
df_for_testing = df_for_testing.dropna(how='any')
print(df_for_testing.shape)

scaler = MinMaxScaler(feature_range=(0,1))   # Zoom data
df_for_training_scaled = scaler.fit_transform(df_for_training)
df_for_training_scaled=df_for_training_scaled[0:test_split,:]
df_for_testing_scaled = scaler.transform(df_for_testing)
df_for_testing_scaled=df_for_testing_scaled[test_split-5:,:]

# createXY
def createXY(dataset,n_past):
    dataX = []
    dataY = []
    for i in range(n_past, len(dataset)):
            dataX.append(dataset[i - 5:i, 0:dataset.shape[1]])  # [0:30,0:5] data in 0-29 days
            dataY.append(dataset[i,0])    # 30 predict the value on the 30th day
    return np.array(dataX),np.array(dataY)
# Generate data
trainX,trainY=createXY(df_for_training_scaled,5)
# trainX.shape
testX,testY=createXY(df_for_testing_scaled,5)

grid_model = keras.Sequential()
grid_model.add(LSTM(30,return_sequences='True',activation='relu',input_shape=(5,5)))
grid_model.add((Dropout(0.2)))
grid_model.add(LSTM(30))
grid_model.add((Dropout(0.2)))
grid_model.add((Dense(1)))
grid_model.summary()

tf.keras.utils.plot_model(model=grid_model,show_shapes=True)

grid_model.compile(loss='mean_squared_error', optimizer="adam")
history = grid_model.fit(trainX,trainY,epochs=200,batch_size=16,validation_data=(testX,testY))

from matplotlib import pyplot as plt
plt.plot(history.history['loss'],label='train')
plt.plot(history.history['val_loss'],label='test6')
plt.legend()
plt.show()
prediction=grid_model.predict(testX)

prediction_copies_array = np.repeat(prediction,5, axis=-1)

pred=scaler.inverse_transform(np.reshape(prediction_copies_array,(len(prediction),5)))[:,0]  # Reverse change, and the standardized data is converted into the original data

# Real data
original_copies_array = np.repeat(testY,5, axis=-1)

# original_copies_array.shape

original=scaler.inverse_transform(np.reshape(original_copies_array,(len(testY),5)))[:,0]
rmse = np.sqrt(np.mean(pred-original)**2)
rmse

prediction1=grid_model.predict(trainX)
prediction_copies_array1 = np.repeat(prediction1,5, axis=-1)
pred1=scaler.inverse_transform(np.reshape(prediction_copies_array1,(len(prediction1),5)))[:,0]
# Real data
original_copies_array1 = np.repeat(trainY,5, axis=-1)

# original_copies_array.shape

original1=scaler.inverse_transform(np.reshape(original_copies_array1,(len(trainY),5)))[:,0]
rmse = np.sqrt(np.mean(pred1-original1)**2)
rmse

plt.plot(original, color = 'red', label = 'Real  Stock Price')
plt.plot(pred, color = 'blue', label = 'Predicted  Stock Price')
plt.title(' Stock Price Prediction')
plt.xlabel('Time')
plt.ylabel(' Stock Price')
plt.legend()
plt.show()

data = prices4.filter(['Close'])
csfont = {'fontname':'Times New Roman'}
train=data[test_split:]
validate = data[test_split:]
validate['prediction']=pred
plt.figure(figsize=(16,8))
plt.xlabel('Date',**csfont,fontsize=18,labelpad=15)
plt.ylabel('Close price',**csfont,fontsize=18,labelpad=15)
plt.plot(validate[['Close']], color='Red')
plt.plot(validate[['prediction']], color='Blue')
plt.legend(['Val','Prediction'],fontsize=10)
plt.show()

train2_df.dtypes

train2_df['Date']=pd.to_datetime(train2_df['Date'])

train2_df

mask =(train2_df['Date']>='2017-01-01')&(train2_df['Date']<='2020-12-30')

train3_df=train2_df.loc[mask]

train3_df

ret_pivot = train3_df.pivot( index='Date', columns='SecuritiesCode',values='Close')

ret_pivot

ret_pivot=ret_pivot.dropna(how='any')

import seaborn as sns

csfont = {'fontname':'Times New Roman'}

ret_pivot.plot(figsize=(20,10),color=['black','yellow','lime','deepskyblue','green','y','orange','orangered','darkred','deeppink','gray','olive','darkviolet','chocolate'])


# Give the axes labels
plt.ylabel('Close Price',**csfont, fontsize=20,labelpad=40)
plt.xlabel('Time',**csfont, fontsize=20,labelpad=40)
plt.legend(loc="upper right",ncol=3)
plt.show()

ret_pivot=ret_pivot.dropna(how='any')

closes =  np.transpose(np.array(ret_pivot)) # matrix of daily closing prices
absdiff = np.diff(closes)        # standard deviation
reldiff = np.divide(absdiff, closes[:,:-1]) # relative change in daily closing price
delta = np.mean(reldiff, axis=1)            # mean price change
sigma = np.cov(reldiff)                     # covariance (standard deviations)
std = np.std(reldiff, axis=1)

import gurobipy as gp
from gurobipy import GRB
from math import sqrt
# Create an empty model
m = gp.Model('portfolio')

# Add matrix variable for the stocks
x = m.addMVar(len(stocks))

# Objective is to minimize risk (squared).  This is modeled using the
# covariance matrix, which measures the historical correlation between stocks
portfolio_risk = x @ sigma @ x
m.setObjective(portfolio_risk, GRB.MINIMIZE)

# Fix budget with a constraint
m.addConstr(x.sum() == 1, 'budget')

# Verify model formulation
m.write('portfolio_selection_optimization.lp')

# Optimize model to find the minimum risk portfolio
m.optimize()

import pandas as pd
minrisk_volatility = sqrt(m.ObjVal)
minrisk_return = delta @ x.X
pd.DataFrame(data=np.append(x.X, [minrisk_volatility, minrisk_return]),
             index=stocks + ['Volatility', 'Expected Return'],
             columns=['Minimum Risk Portfolio'])

# Create an expression representing the expected return for the portfolio
portfolio_return = delta @ x
target = m.addConstr(portfolio_return == minrisk_return, 'target')

# Solve for efficient frontier by varying target return
frontier = np.empty((2,0))
for r in np.linspace(delta.min(), delta.max(), 300):
    target[0].rhs = r
    m.optimize()
    frontier = np.append(frontier, [[sqrt(m.ObjVal)],[r]], axis=1)

import matplotlib.pyplot as plt
#plt.figure(figsize=(10,10))
csfont = {'fontname':'Times New Roman'}
fig, ax = plt.subplots(figsize=(16,8))

# Plot volatility versus expected return for individual stocks
ax.scatter(x=std, y=delta,
           color='Blue', label='Individual Stocks')
for i, stock in enumerate(stocks):
    ax.annotate(stock, (std[i], delta[i]),fontsize=15,**csfont)

# Plot volatility versus expected return for minimum risk portfolio
ax.scatter(x=minrisk_volatility, y=minrisk_return, color='Black')
ax.annotate('Minimum\nRisk\nPortfolio', (minrisk_volatility, minrisk_return),
            horizontalalignment='right',fontsize=15,**csfont)

# Plot efficient frontier
ax.plot(frontier[0], frontier[1], label='Efficient Frontier', color='Red')


# Format and display the final plot
ax.axis([frontier[0].min()*0.71, frontier[0].max()*1.03, delta.min()*0.05, delta.max()*1.2])
ax.set_xlabel('Volatility (standard deviation)',**csfont, fontsize=18,labelpad=40)
ax.set_ylabel('Expected Return',**csfont, fontsize=18,labelpad=40)
ax.tick_params(axis='both',which='major', labelsize=12)

ax.legend()
ax.grid()
plt.show()

train2_df

stocks=['1515','1518','1605','1662','1663','3315','5008','5011','5013','5015','5017','5019','5020','5021']

mask =(train2_df['Date']>='2018-01-01')&(train2_df['Date']<='2018-12-30')
train4_df=train2_df.loc[mask]

ret_pivot1 = train4_df.pivot(index='Date', columns='SecuritiesCode',values='Close')

ret_pivot1
ret_pivot1=ret_pivot1.dropna(how='any')

closes =  np.transpose(np.array(ret_pivot1)) # matrix of daily closing prices
absdiff = np.diff(closes)        # standard deviation
reldiff = np.divide(absdiff, closes[:,:-1]) # relative change in daily closing price
delta = np.mean(reldiff, axis=1)            # mean price change
sigma = np.cov(reldiff)                     # covariance (standard deviations)
std = np.std(reldiff, axis=1)

import gurobipy as gp
from gurobipy import GRB
from math import sqrt
# Create an empty model
m = gp.Model('portfolio')

# Add matrix variable for the stocks
x = m.addMVar(len(stocks))

# Objective is to minimize risk (squared).  This is modeled using the
# covariance matrix, which measures the historical correlation between stocks
portfolio_risk = x @ sigma @ x
m.setObjective(portfolio_risk, GRB.MINIMIZE)

# Fix budget with a constraint
m.addConstr(x.sum() == 1, 'budget')

# Verify model formulation
m.write('portfolio_selection_optimization.lp')

# Optimize model to find the minimum risk portfolio
m.optimize()

import pandas as pd
minrisk_volatility = sqrt(m.ObjVal)
minrisk_return = delta @ x.X
pd.DataFrame(data=np.append(x.X, [minrisk_volatility, minrisk_return]),
             index=stocks + ['Volatility', 'Expected Return'],
             columns=['Minimum Risk Portfolio'])

# Create an expression representing the expected return for the portfolio
portfolio_return = delta @ x
target = m.addConstr(portfolio_return == minrisk_return, 'target')

# Solve for efficient frontier by varying target return
frontier = np.empty((2,0))
for r in np.linspace(delta.min(), delta.max(), 100):
    target[0].rhs = r
    m.optimize()
    frontier = np.append(frontier, [[sqrt(m.ObjVal)],[r]], axis=1)

import matplotlib.pyplot as plt
#plt.figure(figsize=(10,10))
csfont = {'fontname':'Times New Roman'}
fig, ax = plt.subplots(figsize=(17,8))

# Plot volatility versus expected return for individual stocks
ax.scatter(x=std, y=delta,
           color='Blue', label='Individual Stocks')
for i, stock in enumerate(stocks):
    ax.annotate(stock, (std[i], delta[i]),fontsize=15,**csfont)

# Plot volatility versus expected return for minimum risk portfolio
ax.scatter(x=minrisk_volatility, y=minrisk_return, color='Black')
ax.annotate('Minimum\nRisk\nPortfolio', (minrisk_volatility, minrisk_return),
            horizontalalignment='right',fontsize=15,**csfont)

# Plot efficient frontier
ax.plot(frontier[0], frontier[1], label='Efficient Frontier', color='Red')

# Format and display the final plot
ax.axis([frontier[0].min()*0.88, frontier[0].max()*1.1, delta.min()*1.1, delta.max()*1.685])
ax.set_xlabel('Volatility (standard deviation)',**csfont, fontsize=18,labelpad=40)
ax.set_ylabel('Expected Return',**csfont, fontsize=18,labelpad=40)
ax.tick_params(axis='both',which='major', labelsize=12)

ax.legend()
ax.grid()
plt.show()

mask =(train2_df['Date']>'2020-01-01')&(train2_df['Date']<'2020-12-31')
train5_df=train2_df.loc[mask]

ret_pivot2 = train5_df.pivot(index='Date', columns='SecuritiesCode',values='Close')

ret_pivot2=ret_pivot2.dropna(how='any')

closes =  np.transpose(np.array(ret_pivot1)) # matrix of daily closing prices
absdiff = np.diff(closes)        # standard deviation
reldiff = np.divide(absdiff, closes[:,:-1]) # relative change in daily closing price
delta = np.mean(reldiff, axis=1)            # mean price change
sigma = np.cov(reldiff)                     # covariance (standard deviations)
std = np.std(reldiff, axis=1)

import gurobipy as gp
from gurobipy import GRB
from math import sqrt
# Create an empty model
m = gp.Model('portfolio')

# Add matrix variable for the stocks
x = m.addMVar(len(stocks))

# Objective is to minimize risk (squared).  This is modeled using the
# covariance matrix, which measures the historical correlation between stocks
portfolio_risk = x @ sigma @ x
m.setObjective(portfolio_risk, GRB.MINIMIZE)

# Fix budget with a constraint
m.addConstr(x.sum() == 1, 'budget')

# Verify model formulation
m.write('portfolio_selection_optimization.lp')

# Optimize model to find the minimum risk portfolio
m.optimize()

import pandas as pd
minrisk_volatility = sqrt(m.ObjVal)
minrisk_return = delta @ x.X
pd.DataFrame(data=np.append(x.X, [minrisk_volatility, minrisk_return]),
             index=stocks + ['Volatility', 'Expected Return'],
             columns=['Minimum Risk Portfolio'])

# Create an expression representing the expected return for the portfolio
portfolio_return = delta @ x
target = m.addConstr(portfolio_return == minrisk_return, 'target')

# Solve for efficient frontier by varying target return
frontier = np.empty((2,0))
for r in np.linspace(delta.min(), delta.max(), 50):
    target[0].rhs = r
    m.optimize()
    frontier = np.append(frontier, [[sqrt(m.ObjVal)],[r]], axis=1)

import matplotlib.pyplot as plt
#plt.figure(figsize=(10,10))

fig, ax = plt.subplots(figsize=(10,8))

# Plot volatility versus expected return for individual stocks
ax.scatter(x=std, y=delta,
           color='Blue', label='Individual Stocks')
for i, stock in enumerate(stocks):
    ax.annotate(stock, (std[i], delta[i]))

# Plot volatility versus expected return for minimum risk portfolio
ax.scatter(x=minrisk_volatility, y=minrisk_return, color='DarkGreen')
ax.annotate('Minimum\nRisk\nPortfolio', (minrisk_volatility, minrisk_return),
            horizontalalignment='right')

# Plot efficient frontier
ax.plot(frontier[0], frontier[1], label='Efficient Frontier', color='DarkGreen')

# Format and display the final plot
ax.axis([frontier[0].min()*0.7, frontier[0].max()*1.3, delta.min()*1.2, delta.max()*1.2])
ax.set_xlabel('Volatility (standard deviation)')
ax.set_ylabel('Expected Return')
ax.legend()
ax.grid()
plt.show()

import seaborn as sns
X=train2_df.iloc[:,0:15]
y=data.iloc[:,-1]
corrmat=train2_df.corr()
top = corrmat.index
plt.figure(figsize=(15,8))
g=sns.heatmap(train2_df[top].corr(),annot=True,cmap="RdYlGn")